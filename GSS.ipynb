{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f9f2aad-c8c6-4e3f-bd8d-01f7b3a7a3c7",
      "metadata": {
        "id": "4f9f2aad-c8c6-4e3f-bd8d-01f7b3a7a3c7"
      },
      "source": [
        "# U-net Implementation for Guitar String Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4084e0ab-061a-4908-a49a-80fc06cdb34a",
      "metadata": {
        "id": "4084e0ab-061a-4908-a49a-80fc06cdb34a"
      },
      "source": [
        "Firstly, you need to install librosa. If you don't have librosa, make sure you run the requirements.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e633f61-b6e5-48fa-998e-bc8d4124796f",
      "metadata": {
        "execution": {
          "iopub.status.idle": "2023-08-22T14:40:04.606500Z",
          "shell.execute_reply": "2023-08-22T14:40:04.605443Z",
          "shell.execute_reply.started": "2023-08-22T14:40:01.005900Z"
        },
        "id": "0e633f61-b6e5-48fa-998e-bc8d4124796f"
      },
      "outputs": [],
      "source": [
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8e75ca-67cb-43d2-b29e-c0f7674bfc1b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-24T15:37:20.209261Z",
          "iopub.status.busy": "2023-08-24T15:37:20.208465Z",
          "iopub.status.idle": "2023-08-24T15:37:20.547653Z",
          "shell.execute_reply": "2023-08-24T15:37:20.545474Z",
          "shell.execute_reply.started": "2023-08-24T15:37:20.209215Z"
        },
        "id": "1a8e75ca-67cb-43d2-b29e-c0f7674bfc1b"
      },
      "outputs": [],
      "source": [
        "#Importing the essential libraries\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c1ab14-112f-4b9d-bfc9-4ce9f61a3d66",
      "metadata": {
        "id": "09c1ab14-112f-4b9d-bfc9-4ce9f61a3d66"
      },
      "source": [
        "### We are using U-net model which is based on the following two papers:\n",
        "1. \"Singing Voice Separation with Deep U-Net Convolutional Networks\" by Jansson et al., 2017\n",
        "2. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" by Ronneberger et al., 2015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377454f8-657c-41fd-ad3e-f75b60db83ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:08.085231Z",
          "iopub.status.busy": "2023-08-22T14:40:08.084994Z",
          "iopub.status.idle": "2023-08-22T14:40:08.103657Z",
          "shell.execute_reply": "2023-08-22T14:40:08.102662Z",
          "shell.execute_reply.started": "2023-08-22T14:40:08.085205Z"
        },
        "id": "377454f8-657c-41fd-ad3e-f75b60db83ac"
      },
      "outputs": [],
      "source": [
        "# Model Creation\n",
        "\n",
        "def unet(hyper_params,\n",
        "         input=keras.Input((512, 2**7, 1))):\n",
        "  # encoder\n",
        "  x = input\n",
        "  encoder_layers = []\n",
        "  for n_layer in range(hyper_params['NUM_BLOCKS']):\n",
        "    num_filters = 2 ** n_layer * hyper_params['FILTER_MULTIPLIER']\n",
        "    x = keras.layers.Conv2D(num_filters, 5, strides=2, padding='same')(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if hyper_params['DROPOUT_ENCODER'][n_layer]: x = keras.layers.Dropout(hyper_params['DROPOUT_RATE'])(x)\n",
        "    x = keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "    encoder_layers.append(x)\n",
        "\n",
        "  # decoder\n",
        "  for n_layer in range(hyper_params['NUM_BLOCKS']-1):\n",
        "    if not n_layer == 0: x = keras.layers.Concatenate(axis=3)([x, encoder_layers[hyper_params['NUM_BLOCKS'] - 1 - n_layer]])\n",
        "    num_filters = 2 ** (hyper_params['NUM_BLOCKS'] - 1 - n_layer) * hyper_params['FILTER_MULTIPLIER']\n",
        "    x = keras.layers.Conv2DTranspose(num_filters, 5, strides=2, padding='same')(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if hyper_params['DROPOUT_DECODER'][n_layer]: x = keras.layers.Dropout(hyper_params['DROPOUT_RATE'])(x)\n",
        "    x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "  output = keras.layers.Concatenate(axis=3)([x, encoder_layers[-hyper_params['NUM_BLOCKS']]])\n",
        "  output = keras.layers.Conv2DTranspose(1, 5, strides=2, padding='same')(output)\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "\n",
        "  # mask\n",
        "  if hyper_params['MASK']:\n",
        "    output = keras.layers.multiply([input, output])\n",
        "\n",
        "  model = keras.Model(inputs=input, outputs=output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08265286-5ddd-4e07-95b5-07ce26783ec9",
      "metadata": {
        "id": "08265286-5ddd-4e07-95b5-07ce26783ec9"
      },
      "source": [
        "## Computing the Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2370eb03-d71d-4f04-becd-9e17bf8ba471",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:08.107486Z",
          "iopub.status.busy": "2023-08-22T14:40:08.107249Z",
          "iopub.status.idle": "2023-08-22T14:40:08.115351Z",
          "shell.execute_reply": "2023-08-22T14:40:08.114568Z",
          "shell.execute_reply.started": "2023-08-22T14:40:08.107461Z"
        },
        "id": "2370eb03-d71d-4f04-becd-9e17bf8ba471"
      },
      "outputs": [],
      "source": [
        "def compute_SDR_frame(true_frame, predicted_frame):\n",
        "    numerator = np.sum(true_frame**2)\n",
        "    denominator = np.sum((true_frame - predicted_frame)**2)\n",
        "    return 10 * np.log10(numerator / denominator)\n",
        "\n",
        "def compute_SIR_frame(true_frame, predicted_frame):\n",
        "    interference = true_frame - (np.dot(true_frame, predicted_frame) / np.dot(predicted_frame, predicted_frame)) * predicted_frame\n",
        "    numerator = np.sum(true_frame**2)\n",
        "    denominator = np.sum(interference**2)\n",
        "    return 10 * np.log10(numerator / denominator)\n",
        "\n",
        "def compute_SAR_frame(true_frame, predicted_frame):\n",
        "    artifacts = predicted_frame - (np.dot(predicted_frame, true_frame) / np.dot(true_frame, true_frame)) * true_frame\n",
        "    interference = true_frame - (np.dot(true_frame, predicted_frame) / np.dot(predicted_frame, predicted_frame)) * predicted_frame\n",
        "    numerator = np.sum((true_frame + interference)**2)\n",
        "    denominator = np.sum(artifacts**2)\n",
        "    return 10 * np.log10(numerator / denominator)\n",
        "\n",
        "def compute_metric_for_signal(true_signal, predicted_signal, metric_func):\n",
        "    # Reshaping to 2D (num_frames, frame_length)\n",
        "    true_signal = true_signal.reshape(true_signal.shape[0], -1)\n",
        "    predicted_signal = predicted_signal.reshape(predicted_signal.shape[0], -1)\n",
        "\n",
        "    metric_values = [metric_func(true_frame, predicted_frame) for true_frame, predicted_frame in zip(true_signal, predicted_signal)]\n",
        "    return np.mean(metric_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703186e8-6bd5-4715-94bb-048b64a44b1c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:08.116822Z",
          "iopub.status.busy": "2023-08-22T14:40:08.116547Z",
          "iopub.status.idle": "2023-08-22T14:40:08.120844Z",
          "shell.execute_reply": "2023-08-22T14:40:08.119897Z",
          "shell.execute_reply.started": "2023-08-22T14:40:08.116796Z"
        },
        "id": "703186e8-6bd5-4715-94bb-048b64a44b1c"
      },
      "outputs": [],
      "source": [
        "# Initialize the Directory\n",
        "\n",
        "resized_input_dir = \"/notebooks/IN+AUG\"\n",
        "resized_target_dir = \"/notebooks/tar + AUG\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf48838b-ca58-4da0-b7a2-58a6d64e709a",
      "metadata": {
        "id": "cf48838b-ca58-4da0-b7a2-58a6d64e709a"
      },
      "source": [
        "### Checks for Inspecting the Spectrograms to know the mean and the shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae1a1039-cc51-4b84-9466-8d8761c44d22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:31.864252Z",
          "iopub.status.busy": "2023-08-22T14:40:31.863868Z",
          "iopub.status.idle": "2023-08-22T14:40:31.870170Z",
          "shell.execute_reply": "2023-08-22T14:40:31.869276Z",
          "shell.execute_reply.started": "2023-08-22T14:40:31.864225Z"
        },
        "id": "ae1a1039-cc51-4b84-9466-8d8761c44d22"
      },
      "outputs": [],
      "source": [
        "def inspect_saved_spectrograms(spectrogram_dir, limit=5):\n",
        "    \"\"\"\n",
        "    Loads and prints details or content of saved spectrograms.\n",
        "\n",
        "    Parameters:\n",
        "    - spectrogram_dir: Path to directory containing saved spectrogram numpy files.\n",
        "    - limit: Number of spectrogram files to inspect. Default is 5.\n",
        "    \"\"\"\n",
        "    files = sorted(os.listdir(spectrogram_dir))\n",
        "    for i, file in enumerate(files[:limit]):\n",
        "        file_path = os.path.join(spectrogram_dir, file)\n",
        "        spectrogram = np.load(file_path)\n",
        "\n",
        "        print(f\"Spectrogram for {file}:\")\n",
        "        print(f\"Shape: {spectrogram.shape}\")\n",
        "        print(f\"Max: {np.max(spectrogram)}, Min: {np.min(spectrogram)}, Mean: {np.mean(spectrogram)}\")\n",
        "        print(\"\\n\" + \"=\"*40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60920d2e-5e68-486b-b0ca-ae187af94ff5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:32.749256Z",
          "iopub.status.busy": "2023-08-22T14:40:32.748888Z",
          "iopub.status.idle": "2023-08-22T14:40:32.909336Z",
          "shell.execute_reply": "2023-08-22T14:40:32.908315Z",
          "shell.execute_reply.started": "2023-08-22T14:40:32.749228Z"
        },
        "id": "60920d2e-5e68-486b-b0ca-ae187af94ff5",
        "outputId": "de2b322f-ed87-48d9-fe4d-a66787f2631d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spectrogram for 00_BN1-129-Eb_comp_mix.wav.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 22.248661041259766, Min: -57.23155212402344, Mean: -49.51835250854492\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_mix_noise_injection.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 69.12982940673828, Min: -0.23467423021793365, Mean: 0.189961239695549\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_mix_pitch_shifting.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 49.37890625, Min: -0.07383453845977783, Mean: 0.09082947671413422\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_mix_time_stretching.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 64.82308197021484, Min: -0.116488978266716, Mean: 0.08924165368080139\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_solo_mix.wav.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 26.652942657470703, Min: -52.62190246582031, Mean: -42.67133331298828\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_hex_cln.wav.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 15.900431632995605, Min: -64.6517105102539, Mean: -56.013648986816406\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_hex_cln_noise_injection.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 36.26534652709961, Min: -0.13823123276233673, Mean: 0.10162001103162766\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_hex_cln_pitch_shifting.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 33.71566390991211, Min: -0.08647729456424713, Mean: 0.0500447154045105\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_comp_hex_cln_time_stretching.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 28.97545051574707, Min: -0.10162876546382904, Mean: 0.04542741924524307\n",
            "\n",
            "========================================\n",
            "\n",
            "Spectrogram for 00_BN1-129-Eb_solo_hex_cln.wav.npy:\n",
            "Shape: (512, 128, 1)\n",
            "Max: 12.20644474029541, Min: -68.6136474609375, Mean: -58.05252456665039\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inspect_saved_spectrograms(resized_input_dir)\n",
        "inspect_saved_spectrograms(resized_target_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a7dd47-347d-436f-8623-ad4f26b67611",
      "metadata": {
        "id": "84a7dd47-347d-436f-8623-ad4f26b67611"
      },
      "source": [
        "## Learning Rate Finder for efficient model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a3e30bf-e610-4552-8ac7-6b61ce288f75",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:34:34.403237Z",
          "iopub.status.busy": "2023-08-22T14:34:34.402081Z",
          "iopub.status.idle": "2023-08-22T14:34:34.407249Z",
          "shell.execute_reply": "2023-08-22T14:34:34.406632Z",
          "shell.execute_reply.started": "2023-08-22T14:34:34.403207Z"
        },
        "id": "2a3e30bf-e610-4552-8ac7-6b61ce288f75"
      },
      "outputs": [],
      "source": [
        "class LearningRateFinder(keras.callbacks.Callback):\n",
        "    def __init__(self, min_lr=1e-6, max_lr=1e-1, steps_per_epoch=None, epochs=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.total_iterations = steps_per_epoch * epochs\n",
        "        self.iteration = 0\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        \"\"\"Compute the current learning rate.\"\"\"\n",
        "        x = self.iteration / self.total_iterations\n",
        "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        \"\"\"Initialize the learning rate to the minimum value at the start of training.\"\"\"\n",
        "        logs = logs or {}\n",
        "        keras.backend.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \"\"\"Record the previous batch's statistics and update the learning rate.\"\"\"\n",
        "        logs = logs or {}\n",
        "        self.iteration += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(keras.backend.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.iteration)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "\n",
        "def plot_lr_finder(history):\n",
        "    \"\"\"Plot the loss versus learning rate.\"\"\"\n",
        "    plt.plot(history['lr'], history['loss'])\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Learning Rate Finder')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e2bb4c-29cd-43cd-90d5-b7ef687ca14e",
      "metadata": {
        "id": "09e2bb4c-29cd-43cd-90d5-b7ef687ca14e"
      },
      "source": [
        "## Normalize your spectrograms if not already normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3828ca-d109-4a89-a1d8-412d83e15341",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:34:35.269684Z",
          "iopub.status.busy": "2023-08-22T14:34:35.268523Z",
          "iopub.status.idle": "2023-08-22T14:34:35.275723Z",
          "shell.execute_reply": "2023-08-22T14:34:35.274651Z",
          "shell.execute_reply.started": "2023-08-22T14:34:35.269636Z"
        },
        "id": "cc3828ca-d109-4a89-a1d8-412d83e15341"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_spectrogram(spectrogram):\n",
        "    \"\"\"\n",
        "    Normalize the given spectrogram.\n",
        "\n",
        "    :param spectrogram: numpy array of the spectrogram\n",
        "    :return: normalized spectrogram\n",
        "    \"\"\"\n",
        "    # Subtract the mean and divide by the standard deviation\n",
        "    normalized_spectrogram = (spectrogram - np.mean(spectrogram)) / np.std(spectrogram)\n",
        "    return normalized_spectrogram\n",
        "\n",
        "\n",
        "def normalize_spectrograms(input_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Normalize all spectrograms in the input directory and save to the output directory.\n",
        "\n",
        "    :param input_dir: directory containing the original spectrograms\n",
        "    :param output_dir: directory where normalized spectrograms will be saved\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for file_name in os.listdir(input_dir):\n",
        "        if file_name.endswith('.npy'):\n",
        "            file_path = os.path.join(input_dir, file_name)\n",
        "            spectrogram = np.load(file_path)\n",
        "            normalized_spectrogram = normalize_spectrogram(spectrogram)\n",
        "            output_path = os.path.join(output_dir, file_name)\n",
        "            np.save(output_path, normalized_spectrogram)\n",
        "\n",
        "\n",
        "# Example usage: for mix\n",
        "# input_directory = \"/notebooks/mix/combined_mix_resize\"\n",
        "# output_directory = \"/notebooks/mix/norm_mix_resz\"\n",
        "# normalize_spectrograms(input_directory, output_directory)\n",
        "\n",
        "\n",
        "# input_directory_hex = \"/notebooks/hex/combined_hex_resize\"\n",
        "# output_directory_hex = \"/notebooks/hex/norm_hex_resz\"\n",
        "# normalize_spectrograms(input_directory_hex, output_directory_hex)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "200b17e2-8b18-4b79-af15-dc372190338f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:55.122182Z",
          "iopub.status.busy": "2023-08-22T14:40:55.121849Z",
          "iopub.status.idle": "2023-08-22T14:40:55.130291Z",
          "shell.execute_reply": "2023-08-22T14:40:55.129330Z",
          "shell.execute_reply.started": "2023-08-22T14:40:55.122157Z"
        },
        "id": "200b17e2-8b18-4b79-af15-dc372190338f"
      },
      "outputs": [],
      "source": [
        "input_files = sorted(os.listdir(resized_input_dir))\n",
        "target_files = sorted(os.listdir(resized_target_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a99314-a385-4fdf-a2e2-0cf546d19821",
      "metadata": {
        "id": "93a99314-a385-4fdf-a2e2-0cf546d19821"
      },
      "source": [
        "## Some sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c8d400-a605-47b6-8838-92d17428ad53",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:40:55.519878Z",
          "iopub.status.busy": "2023-08-22T14:40:55.519497Z",
          "iopub.status.idle": "2023-08-22T14:41:15.275535Z",
          "shell.execute_reply": "2023-08-22T14:41:15.274470Z",
          "shell.execute_reply.started": "2023-08-22T14:40:55.519852Z"
        },
        "id": "f2c8d400-a605-47b6-8838-92d17428ad53"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to determine the maximum shape from a list of .npy files\n",
        "def determine_max_shape(files, directory):\n",
        "    max_shape = (0, 0)\n",
        "    for f in files:\n",
        "        data_shape = np.load(os.path.join(directory, f)).shape\n",
        "        max_shape = (max(max_shape[0], data_shape[0]), max(max_shape[1], data_shape[1]))\n",
        "    return max_shape\n",
        "\n",
        "# Function to load and pad the numpy arrays\n",
        "def load_and_pad(file_path, max_shape):\n",
        "    data = np.load(file_path)\n",
        "    padding = [(0, max_shape[i] - data.shape[i]) for i in range(2)] + [(0,0)]  # the last padding is for the channel dimension\n",
        "    return np.pad(data, padding)\n",
        "\n",
        "max_input_shape = determine_max_shape(input_files, resized_input_dir)\n",
        "max_target_shape = determine_max_shape(target_files, resized_target_dir)\n",
        "# Here, I'm assuming that max_input_shape is same as max_target_shape,\n",
        "# if that's not the case you might need to pad them to a common maximum shape\n",
        "\n",
        "def data_generator(input_files, target_files, batch_size=32, max_shape=max_input_shape):\n",
        "    while True:\n",
        "        for i in range(0, len(input_files), batch_size):\n",
        "            input_batch = np.array([load_and_pad(os.path.join(resized_input_dir, f), max_shape) for f in input_files[i:i+batch_size]])\n",
        "            target_batch = np.array([load_and_pad(os.path.join(resized_target_dir, f), max_shape) for f in target_files[i:i+batch_size]])\n",
        "            yield (input_batch, target_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a45ad51-57bb-412c-8259-742e8d316e91",
      "metadata": {
        "id": "8a45ad51-57bb-412c-8259-742e8d316e91"
      },
      "source": [
        "Making sure that the directories are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e38712-1523-465a-9c60-c9fe87ac7510",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:41:17.168127Z",
          "iopub.status.busy": "2023-08-22T14:41:17.167785Z",
          "iopub.status.idle": "2023-08-22T14:41:17.186347Z",
          "shell.execute_reply": "2023-08-22T14:41:17.185535Z",
          "shell.execute_reply.started": "2023-08-22T14:41:17.168100Z"
        },
        "id": "48e38712-1523-465a-9c60-c9fe87ac7510",
        "outputId": "547141f0-39fa-4019-fc54-6a605aed6490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in /notebooks/IN+AUG: 1439\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def count_files_in_directory(directory):\n",
        "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
        "\n",
        "directory_path = resized_input_dir  # Change this to the path of your directory\n",
        "print(f\"Number of files in {directory_path}: {count_files_in_directory(directory_path)}\")\n",
        "\n",
        "directory_path = resized_target_dir  # Change this to the path of your directory\n",
        "print(f\"Number of files in {directory_path}: {count_files_in_directory(directory_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2263c17-531f-4dd9-a297-2f2a65d47ca7",
      "metadata": {
        "id": "a2263c17-531f-4dd9-a297-2f2a65d47ca7"
      },
      "source": [
        "## Defining Train/Validation/Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97605858-547b-4fe6-b375-4a4eede3b1dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:41:19.143008Z",
          "iopub.status.busy": "2023-08-22T14:41:19.142020Z",
          "iopub.status.idle": "2023-08-22T14:41:19.150044Z",
          "shell.execute_reply": "2023-08-22T14:41:19.149334Z",
          "shell.execute_reply.started": "2023-08-22T14:41:19.142981Z"
        },
        "id": "97605858-547b-4fe6-b375-4a4eede3b1dd"
      },
      "outputs": [],
      "source": [
        "indices = np.arange(len(input_files))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_split = int(0.8 * len(input_files))\n",
        "val_split = int(0.9 * len(input_files))\n",
        "\n",
        "train_indices = indices[:train_split]\n",
        "val_indices = indices[train_split:val_split]\n",
        "test_indices = indices[val_split:]\n",
        "\n",
        "train_input_files = [input_files[i] for i in train_indices]\n",
        "train_target_files = [target_files[i] for i in train_indices]\n",
        "val_input_files = [input_files[i] for i in val_indices]\n",
        "val_target_files = [target_files[i] for i in val_indices]\n",
        "test_input_files = [input_files[i] for i in test_indices]\n",
        "test_target_files = [target_files[i] for i in test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75219ad-69db-422f-81ae-0f913c384384",
      "metadata": {
        "id": "c75219ad-69db-422f-81ae-0f913c384384"
      },
      "source": [
        "## The following are the model Hyperparameters.\n",
        "What each parameter does is described below.\n",
        "\n",
        "\n",
        "- NUM_BLOCKS -> Blocks in the encoder and decoder, for example, if NUM_BLOCKS is set to 6, there are 6 blocks in the encoder and 6 blocks in the decoder. 2D convolution, batch normalisation, and leaky ReLU layers make up each encoder block. Each decoder block includes batch normalisation, transposed 2D convolution,  potentially dropout, and ReLU layers\n",
        "- FILTER_MULTIPLIER -> Sets the number of filters in 2D convolution layers.\n",
        "- DROPOUT_ENCODER -> A list of boolean values of lengths equal to the number of blocks in the encoder. Each boolean value corresponds to a block and decides if that block should have dropout or not.\n",
        "- DROPOUT_DECODER -> A list of boolean values of lengths equal to the number of blocks in the decoder. Each boolean value corresponds to a block and decides if that block should have dropout or not.\n",
        "- DROPOUT_RATE -> Set the dropout rate in the dropout layers.\n",
        "- MASK -> Estimate a mask (and multiply it with the input) instead of estimating the output directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c294d2-8703-4793-ab7c-cc53f44568be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:41:19.896755Z",
          "iopub.status.busy": "2023-08-22T14:41:19.895657Z",
          "iopub.status.idle": "2023-08-22T14:41:21.566264Z",
          "shell.execute_reply": "2023-08-22T14:41:21.565510Z",
          "shell.execute_reply.started": "2023-08-22T14:41:19.896726Z"
        },
        "id": "37c294d2-8703-4793-ab7c-cc53f44568be"
      },
      "outputs": [],
      "source": [
        "hyper_params = {\n",
        "    'NUM_BLOCKS': 6,\n",
        "    'FILTER_MULTIPLIER': 16,\n",
        "    'DROPOUT_ENCODER': [False, False, True, True, True, True],\n",
        "    'DROPOUT_DECODER': [False, False, True, True, True, True],\n",
        "    'DROPOUT_RATE': 0.5,\n",
        "    'MASK': True\n",
        "}\n",
        "\n",
        "model = unet(hyper_params)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaec672-8ac1-4094-88d1-842e808046d9",
      "metadata": {
        "id": "fdaec672-8ac1-4094-88d1-842e808046d9"
      },
      "source": [
        "## Small test to identify the best learning rate.\n",
        "NOTE: Rerun the model after finding the appropriate LR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfaf7730-75ab-4918-b158-b0e9dd628d26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:41:22.349309Z",
          "iopub.status.busy": "2023-08-22T14:41:22.348417Z",
          "iopub.status.idle": "2023-08-22T14:41:22.353504Z",
          "shell.execute_reply": "2023-08-22T14:41:22.352667Z",
          "shell.execute_reply.started": "2023-08-22T14:41:22.349281Z"
        },
        "id": "bfaf7730-75ab-4918-b158-b0e9dd628d26"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "steps_per_epoch = len(train_input_files) // batch_size\n",
        "validation_steps = len(val_input_files) // batch_size\n",
        "train_gen = data_generator(train_input_files, train_target_files, batch_size)\n",
        "val_gen = data_generator(val_input_files, val_target_files, batch_size)\n",
        "\n",
        "\n",
        "# Initialize Learning Rate Finder\n",
        "lr_finder = LearningRateFinder(min_lr=1e-6, max_lr=1e-1, steps_per_epoch=steps_per_epoch, epochs=1)\n",
        "\n",
        "# Using only 1 epoch for the Learning Rate Finder. You can use more if you want a more detailed curve.\n",
        "model.fit(train_gen, epochs=5, steps_per_epoch=steps_per_epoch, callbacks=[lr_finder])\n",
        "\n",
        "# Plot the loss against learning rates\n",
        "plot_lr_finder(lr_finder.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27268e93-28b5-48df-8908-240a81e6f0dc",
      "metadata": {
        "id": "27268e93-28b5-48df-8908-240a81e6f0dc"
      },
      "source": [
        "## Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71abdf2a-50ce-4f9b-9954-ffde06173fbb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:41:46.752427Z",
          "iopub.status.busy": "2023-08-22T14:41:46.751727Z",
          "iopub.status.idle": "2023-08-22T14:51:06.541074Z",
          "shell.execute_reply": "2023-08-22T14:51:06.540164Z",
          "shell.execute_reply.started": "2023-08-22T14:41:46.752400Z"
        },
        "id": "71abdf2a-50ce-4f9b-9954-ffde06173fbb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "batch_size = 32\n",
        "steps_per_epoch = len(train_input_files) // batch_size\n",
        "validation_steps = len(val_input_files) // batch_size\n",
        "train_gen = data_generator(train_input_files, train_target_files, batch_size)\n",
        "val_gen = data_generator(val_input_files, val_target_files, batch_size)\n",
        "\n",
        "# Set up ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Add Early Stopping callback\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n",
        "model.fit(train_gen,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=100,\n",
        "          validation_data=val_gen,\n",
        "          validation_steps=validation_steps,\n",
        "          callbacks = early_stopping, reduce_lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a33df1-84bf-4ea8-a2ca-f988f269da5e",
      "metadata": {
        "id": "60a33df1-84bf-4ea8-a2ca-f988f269da5e"
      },
      "source": [
        "## Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dd11ca-fa7e-47ec-afff-6363083fffbb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:51:18.242318Z",
          "iopub.status.busy": "2023-08-22T14:51:18.241931Z",
          "iopub.status.idle": "2023-08-22T14:51:18.639067Z",
          "shell.execute_reply": "2023-08-22T14:51:18.637772Z",
          "shell.execute_reply.started": "2023-08-22T14:51:18.242293Z"
        },
        "id": "64dd11ca-fa7e-47ec-afff-6363083fffbb"
      },
      "outputs": [],
      "source": [
        "test_gen = data_generator(test_input_files, test_target_files, batch_size)\n",
        "loss = model.evaluate(test_gen, steps=len(test_input_files) // batch_size)\n",
        "print(f\"Test Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f71166-7810-4715-967e-361546f67081",
      "metadata": {
        "id": "68f71166-7810-4715-967e-361546f67081"
      },
      "source": [
        "## Main Evaluation Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a8b1b6-92f3-4047-bfd7-ab90d4d8e64b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-22T14:51:20.315092Z",
          "iopub.status.busy": "2023-08-22T14:51:20.314728Z",
          "iopub.status.idle": "2023-08-22T14:51:25.774549Z",
          "shell.execute_reply": "2023-08-22T14:51:25.773713Z",
          "shell.execute_reply.started": "2023-08-22T14:51:20.315068Z"
        },
        "id": "02a8b1b6-92f3-4047-bfd7-ab90d4d8e64b"
      },
      "outputs": [],
      "source": [
        "SDRs, SIRs, SARs = [], [], []\n",
        "\n",
        "# Iterate over test dataset to compute the metrics:\n",
        "for i, (inputs, targets) in enumerate(test_gen):\n",
        "    if i == len(test_input_files) // batch_size:\n",
        "        break\n",
        "    predictions = model.predict(inputs)\n",
        "    for true_signal, predicted_signal in zip(targets, predictions):\n",
        "        SDRs.append(compute_metric_for_signal(true_signal, predicted_signal, compute_SDR_frame))\n",
        "        SIRs.append(compute_metric_for_signal(true_signal, predicted_signal, compute_SIR_frame))\n",
        "        SARs.append(compute_metric_for_signal(true_signal, predicted_signal, compute_SAR_frame))\n",
        "\n",
        "# Calculate average metrics for the entire test set:\n",
        "average_SDR = np.mean(SDRs)\n",
        "average_SIR = np.mean(SIRs)\n",
        "average_SAR = np.mean(SARs)\n",
        "\n",
        "print(f\"Average Test SDR: {average_SDR:.4f}\")\n",
        "print(f\"Average Test SIR: {average_SIR:.4f}\")\n",
        "print(f\"Average Test SAR: {average_SAR:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}